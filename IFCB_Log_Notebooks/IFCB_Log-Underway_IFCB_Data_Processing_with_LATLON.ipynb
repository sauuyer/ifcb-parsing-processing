{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy #run this line if geopy module is not found\n",
    "#!pip install chardet #run this line if chardet module is not found\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from geopy.distance import geodesic\n",
    "import chardet #character encoding detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad3fec",
   "metadata": {},
   "source": [
    "### Processing ship underway data to obtain lat and lon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc56de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS THE ONLY CELL CONTAINING ELEMENTS THAT NEED TO BE CHANGED\n",
    "    # 1) the directory containing leg 1 underway data\n",
    "    # 2) the directory containing leg 2 underway data\n",
    "    # 3) the cruise number - this is just used for the csv name saved at the end of the notebook\n",
    "\n",
    "#the directory containing the csv files from the underway system\n",
    "##each file is a day, and the file is data collected every minute\n",
    "dir_leg_1 = \"/AR82-Pioneer20/AR82_armstrong_underway_data/leg1/proc\" # change me (1)\n",
    "dir_leg_2 = \"/AR82-Pioneer20/AR82_armstrong_underway_data/leg2/proc\" # change me (2)\n",
    "cruise_name = \"AR82\" # change me (3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the regex pattern that will match the correct underway files\n",
    "file_name_pattern = r'AR\\d*_0000.csv'\n",
    "#compile the regex pattern\n",
    "pattern_regex = re.compile(file_name_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all csv files in the directory matching the pattern above\n",
    "leg_1_csv_files = [file for file in os.listdir(dir_leg_1) if pattern_regex.match(file)]\n",
    "leg_2_csv_files = [file for file in os.listdir(dir_leg_2) if pattern_regex.match(file)]\n",
    "\n",
    "#empty list to store individual dataframes before merge\n",
    "dfs = []\n",
    "\n",
    "#iterate over each csv file, read as df, and append to dfs list\n",
    "for file in leg_1_csv_files:\n",
    "    file_path = os.path.join(dir_leg_1, file)\n",
    "    df = pd.read_csv(file_path, header=1)\n",
    "    dfs.append(df)\n",
    "    \n",
    "for file in leg_2_csv_files:\n",
    "    file_path = os.path.join(dir_leg_2, file)\n",
    "    df = pd.read_csv(file_path, header=1)\n",
    "    dfs.append(df)\n",
    "\n",
    "#concatenate all dfs in the list into a single df\n",
    "merged_ship_underway_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ship_underway_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime format\n",
    "def gmt_to_utc(gmt_datetime_str):\n",
    "    gmt = pytz.timezone('GMT')\n",
    "    datetime_formats = [\n",
    "        '%Y/%m/%d %H:%M:%S',\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%Y/%m/%d %H:%M:%S.%f'\n",
    "    ]\n",
    "\n",
    "    for fmt in datetime_formats:\n",
    "        try:\n",
    "            gmt_datetime = datetime.strptime(gmt_datetime_str, fmt)\n",
    "            gmt_localized_datetime = gmt.localize(gmt_datetime)\n",
    "            utc_datetime = gmt_localized_datetime.astimezone(pytz.utc)\n",
    "            return utc_datetime\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Time data '{gmt_datetime_str}' does not match any known format\")\n",
    "\n",
    "merged_underway_df['Datetime_UTC_underway'] = merged_underway_df['Datetime_GMT_underway'].apply(gmt_to_utc)\n",
    "merged_underway_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd00836",
   "metadata": {},
   "source": [
    "### Processing raw underway IFCB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory where the underway IFCB Data is stored\n",
    "dir = \"/Volumes/My Passport/IFCB_PLIMS/IFCB_Cruise_data/Pioneer21_AR87_IFCB_data/AR87_shipboard_underway_ifcb200\"\n",
    "\n",
    "# indicate the file parameters to target (these are the columns needed in the IFCB log)\n",
    "\n",
    "file_parameters_from_hdr_files = [\"FileComment\", \"triggerCount\", \"roiCount\", \"runTime\", \"inhibitTime\", \"SyringeSampleVolume\", \"syringeSamplingSpeed\", \"temperature\", \"RunFastFactor\",\n",
    "                   \"sampleNumber\", \"runSampleFast\"]\n",
    "\n",
    "# dictionary used to organize the filenames/sample ids with the associated parameters\n",
    "all_fileparam_dicts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71699e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECLARE the FUNCTION USED for PATTERN MATCHING within the HEADER FILE\n",
    "\n",
    "def gather_values(text_content, param_list):\n",
    "    file_dict = {\"Filename\": filename}\n",
    "    for p in param_list:\n",
    "        escaped_dynamic_string = re.escape(p)\n",
    "        pattern = re.compile(r'{}:\\s*(.+)'.format(escaped_dynamic_string))\n",
    "        dynamic_match = pattern.search(hdr_content)\n",
    "\n",
    "        if dynamic_match:\n",
    "            value = dynamic_match.group(1)\n",
    "            #return dynamic_match.group(1)\n",
    "            #print(p, \" value: \", value)\n",
    "            file_dict[p] = value\n",
    "        else: #return None\n",
    "            print(filename, \"No match found.\")\n",
    "\n",
    "    all_fileparam_dicts.append(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7505f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### APPLY gather_values() to ALL HDR FILES in the ESTABLISHED dir\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith(\".hdr\") and not file.startswith(\"._\"):\n",
    "        filename = os.path.splitext(file)[0]\n",
    "        filepath = os.path.join(dir, file)\n",
    "        with open(filepath, \"r\", encoding=\"iso-8859-1\") as f:\n",
    "            hdr_content = f.read()\n",
    "            #print(hdr_content) # read out of all hdr content\n",
    "            gather_values(hdr_content, file_parameters_from_hdr_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "underway_IFCB_hdr_output = pd.DataFrame(all_fileparam_dicts)\n",
    "underway_IFCB_hdr_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cffdf",
   "metadata": {},
   "source": [
    "### Adding volume analyzed (& lookTime, flowRate, and runSampleFast_Int) to the output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "underway_IFCB_output_with_calcs = underway_IFCB_hdr_output\n",
    "\n",
    "# Pull date and time from the filename\n",
    "underway_IFCB_output_with_calcs['Datetime'] = pd.to_datetime(underway_IFCB_output_with_calcs['Filename'].str[1:15], format='%Y%m%dT%H%M%S')\n",
    "\n",
    "# Ensure needed numeric values are numeric and not strings\n",
    "underway_IFCB_output_with_calcs['runTime'] = pd.to_numeric(underway_IFCB_output_with_calcs['runTime'], errors='coerce')\n",
    "underway_IFCB_output_with_calcs['inhibitTime'] = pd.to_numeric(underway_IFCB_output_with_calcs['inhibitTime'], errors='coerce')\n",
    "underway_IFCB_output_with_calcs['syringeSamplingSpeed'] = pd.to_numeric(underway_IFCB_output_with_calcs['syringeSamplingSpeed'], errors='coerce')\n",
    "underway_IFCB_output_with_calcs['SyringeSampleVolume'] = pd.to_numeric(underway_IFCB_output_with_calcs['SyringeSampleVolume'], errors='coerce')\n",
    "underway_IFCB_output_with_calcs['RunFastFactor'] = pd.to_numeric(underway_IFCB_output_with_calcs['RunFastFactor'], errors='coerce')\n",
    "\n",
    "\n",
    "# Create a column for lookTime\n",
    "underway_IFCB_output_with_calcs['lookTime'] = underway_IFCB_output_with_calcs['runTime'] - underway_IFCB_output_with_calcs['inhibitTime']\n",
    "\n",
    "\n",
    "# Create a column for runSampleFast_Int\n",
    "## if runsamplefast = false then runsamplefast==1\n",
    "underway_IFCB_output_with_calcs['runSampleFast_Int'] = (underway_IFCB_output_with_calcs['runSampleFast'].str.lower() != 'true').astype(int)\n",
    "underway_IFCB_output_with_calcs['runSampleFast_Int']\n",
    "\n",
    "    \n",
    "# Create a column for flowRate_mins ()\n",
    "## syringeSamplingSpeed (usually 20 mins), SyringeSampleVolume (usually 5 ml)\n",
    "underway_IFCB_output_with_calcs['flowRate_mins'] = underway_IFCB_output_with_calcs['SyringeSampleVolume'] / underway_IFCB_output_with_calcs['syringeSamplingSpeed']\n",
    "\n",
    "\n",
    "# Create a column for volumeAnalyzed\n",
    "underway_IFCB_output_with_calcs['volumeAnalyzed'] = (underway_IFCB_output_with_calcs['RunFastFactor'] * underway_IFCB_output_with_calcs['runSampleFast_Int']) * underway_IFCB_output_with_calcs['flowRate_mins'] * (underway_IFCB_output_with_calcs['lookTime']/60)/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "underway_IFCB_output_with_calcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff4a13",
   "metadata": {},
   "source": [
    "### Adding ship latitude and longitude values to IFCB output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65958a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the name of the underway ship data df created above within this notebook: merged_ship_underway_df\n",
    "\n",
    "merged_ship_underway_df['Datetime_GMT_underway'] = pd.to_datetime(merged_ship_underway_df['Datetime_GMT_underway'], errors='coerce')\n",
    "merged_ship_underway_df['Datetime_GMT_underway'].dtype\n",
    "merged_ship_underway_df\n",
    "\n",
    "# Uncomment these two following lines if you want to look at faulty/missing dates that might be present in the ship data, these can appear during GPS outages\n",
    "#invalid_dates = underway_ship_data[underway_ship_data['Datetime_GMT_underway'].isna()]\n",
    "#print(invalid_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876b946",
   "metadata": {},
   "source": [
    "### Join the lat/lon values from the ship system data to the HDR plus calcs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both dfs by date\n",
    "underway_IFCB_output_with_calcs = underway_IFCB_output_with_calcs.sort_values('Datetime')\n",
    "merged_ship_underway_df = merged_ship_underway_df.sort_values('Datetime_GMT_underway')\n",
    "\n",
    "# Drop any rows in the ship data that do not have datetimes\n",
    "merged_ship_underway_df = merged_ship_underway_df.dropna(subset=['Datetime_GMT_underway'])\n",
    "\n",
    "# Merge the underway ship data to the IFCB HDR summary df\n",
    "hdr_summary_with_lat_and_lon = pd.merge_asof(underway_IFCB_output_with_calcs, merged_ship_underway_df, left_on='Datetime', right_on='Datetime_GMT_underway', direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the df containing all hdr summary information and ship lat and lon\n",
    "hdr_summary_with_lat_and_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99728111",
   "metadata": {},
   "source": [
    "### Identifying and adding OOI sites to the df using ship lat/lon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc68d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mab_site_centers = {\n",
    "    'site': ['CP10CNSM', 'CP11NOSM', 'CP11SOSM', 'CP12CNSW', 'CP12WESW', 'CP13EAPM', \n",
    "             'CP13NOPM', 'CP13SOPM', 'CP14NEPM', 'CP14SEPM'],\n",
    "    'lat': [35.95, 36.175, 35.725, 35.95, 35.95, 35.95, 36.175, 35.725, 36.0536, 35.8514],\n",
    "    'lon': [-75.125, -74.8267, -74.853, -75.125, -75.3333, -74.8457, -74.8267, -74.853, \n",
    "            -74.7776, -74.8482]\n",
    "}\n",
    "\n",
    "centers_df = pd.DataFrame(mab_site_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_summary_with_lat_and_lon.columns.tolist()\n",
    "#hdr_summary_with_lat_and_lon[\" Dec_LON\"] - this was used for AR82, but this col is not in the AR87 files\n",
    "hdr_summary_with_lat_and_lon[' CNAV_LON']\n",
    "hdr_summary_with_lat_and_lon[' CNAV_LAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifcb_lat = hdr_summary_with_lat_and_lon[\" CNAV_LAT\"]\n",
    "ifcb_lon = hdr_summary_with_lat_and_lon[\" CNAV_LON\"]\n",
    "\n",
    "hdr_summary_with_lat_and_lon_and_sites = hdr_summary_with_lat_and_lon\n",
    "\n",
    "def check_within_radius(ifcb_lat, ifcb_lon, centers_df, radius):\n",
    "    for _, center in centers_df.iterrows():\n",
    "        center_lat = center['lat']\n",
    "        center_lon = center['lon']\n",
    "        center_name = center.get('site', 'Unnamed Site')\n",
    "\n",
    "        # Calculate distance\n",
    "        distance = geodesic((ifcb_lat, ifcb_lon), (center_lat, center_lon)).kilometers\n",
    "        if distance <= radius:\n",
    "            return True, center_name  # Return True and the site name if within radius\n",
    "    return False, None  # Return False if not within any radius\n",
    "\n",
    "# Define a wrapper function to apply row-wise\n",
    "def apply_check_within_radius(row):\n",
    "    return check_within_radius(row[' CNAV_LAT'], row[' CNAV_LON'], centers_df, radius=2)  # radius in kilometers\n",
    "\n",
    "\n",
    "\n",
    "# Apply function to every row in the DataFrame\n",
    "hdr_summary_with_lat_and_lon_and_sites[['within_radius', 'site_name']] = hdr_summary_with_lat_and_lon_and_sites.apply(apply_check_within_radius, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78255bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_summary_with_lat_and_lon_and_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE this VERSION of the DF CONTAINING ALL SUMMARY DATA, LAT, LON, and SITE NAMES\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "output_filename = f\"HDR_Summaries/underway_ifcb_hdr_summaries/{cruise_name}_ifcb_underway_hdr_summary_with_lat_lon_and_sites_{timestamp}.csv\"\n",
    "hdr_summary_with_lat_and_lon_and_sites.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
